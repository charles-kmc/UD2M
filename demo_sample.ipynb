{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0fab04",
   "metadata": {},
   "source": [
    "## Learning Few-Step Posterior Samplers by Unfolding and Distillation of Diffusion Models\n",
    "### Demo (Sampling a distilled model for gaussian deblurring)\n",
    "\n",
    "#### Introduction\n",
    "This notebook provides a demonstration of generating posterior samples $x\\sim p(x|y)$ from a trained unfolded distilled conditional diffusion model UD$^2$M, as discussed in the paper [Learning Few-Step Posterior Samplers by Unfolding and Distillation of Diffusion Models](https://arxiv.org/abs/2507.02686). This demo considers the relation problem $y = k* x + \\mathcal{N}(0, \\sigma^2)$ for an anisotropic Gaussian filter $k$, applied to a model trained on LSUN bedroom training data.\n",
    "\n",
    "The method uses a DDIM discretization to approximate a reversed diffusion process, where the conditional score given $y$ and $x_t$ is approximated through estimated samples from the joint posterior $\\hat x_{0,t} \\sim p(x_0|y, x_t)$ for a decreasing sequence of values $t$, where $x_t \\sim \\mathcal N(\\sqrt{\\overline{\\alpha}_t}x, {1 - \\overline{\\alpha}_t})$.\n",
    "\n",
    "![alt text](./demo/figs/alg1.png \"UDDM Algorithm\")\n",
    "\n",
    "The samples $\\hat x_{0,t} = x_0^{(K)}$ are obtained by unrolling $K$ iterations of the LATINO algorithm:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde x_{0, t}^{(k+1)} &= \\textnormal{prox}_{-\\delta\\log p(y, x_t|\\cdot)}(x_{0,t}^{(k)})\\\\\n",
    "    x_{t_\\delta}^{(k+1)} &= \\sqrt{\\overline{\\alpha}_{t_\\delta}}x_{0,t}^{(k+1)} + \\sqrt{1 - \\overline{\\alpha}_{t_\\delta}}\\mathcal{N}(0, I)\\\\\n",
    "    x_{0, t}^{(k+1)} &= \\tilde G_\\vartheta(x_{t_\\delta}^{(k+1)}, t_\\delta),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\tilde G_\\theta$ is a denoiser which has been pre-trained by diffusion score matching on images close to the prior distribution $p(x)$. This algorithm is unfolded, with trainable parameters:\n",
    "\n",
    "- $\\Delta\\theta$ such that $\\vartheta = \\theta + \\Delta\\theta$, $\\Delta \\theta$ represents a low-rank perturbation of the (large) pre-trained denoiser $\\theta$. Using [LoRA](https://arxiv.org/abs/2106.09685), we focus on learning low rank perturbations of the attention layers of the denoiser.\n",
    "- The step size $\\delta$.\n",
    "- The level of augmented noise added before each denoiser step, $t_\\delta$.\n",
    "\n",
    "To test different initializations $x_{0,t}^{(0)}$ of the unrolled sampling architecture, we consider the following schemes:\n",
    "- Start from $y$: $x_{0,t}^{(0)} = y$.\n",
    "- Using the Reconstruct anything model [(RAM)](https://arxiv.org/abs/2503.08915) applied to the joint observation $y, x_t$: $x_{0,t}^{(0)} = \\textnormal{RAM}(y, x_t)$. In this setup, the RAM weights are fine-tuned as part of the unrolled sampling architecture.\n",
    "\n",
    "#### Training \n",
    "The unrolled sampling weights are trained using the procedure described in Section 3.2 of the paper. To summarize, the loss takes the form \n",
    "$$\n",
    "    \\mathcal{L} = \\mathcal{L}_{\\text{Adv}, \\phi} + \\omega_1 \\mathcal L_{\\text{MSE}} + \\omega_2 \\mathcal L_{\\text{PS}},\n",
    "$$\n",
    "where $\\mathcal{L}_{\\text{Adv}, \\phi}$ is an adversarial loss based on the Jensen-Shannon divergence, $\\mathcal L_{\\text{MSE}}$ is a mean squared error loss, and $\\mathcal L_{\\text{PS}}$ is a perceptual similarity loss. $\\omega_1$ and $\\omega_2$ are hyper-parameters used to stabilize training. The adversarial loss is computed between real and fake samples of the joint distribution $(x, y)$ using a discriminator with weights $\\phi$, which are trained jointly with the unrolled sampling weights $\\vartheta$. To avoid overfitting, a gradient penalty is applied to the discriminator loss.\n",
    "#### Setup\n",
    "The dependencies can be installed using \n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Download the pretrained diffusion UNet model weights from [here](https://heibox.uni-heidelberg.de/d/01207c3f6b8441779abf/?p=%2Fdiffusion_models_converted%2Fdiffusion_lsun_bedroom_model&mode=list) and save them as `./model_zoo/diffusion_lsun_bedroom_model-2388000.ckpt`.\n",
    "\n",
    "Download the UDDM model fine-tuned weights from [here](https://drive.google.com/file/d/17bC8nSU7Sd6M_eTZh37T3wQJLo4RvyQh/view?usp=sharing) and save them as `./demo_models/LSUN_RAM_gaussian_deblur.ckpt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports \n",
    "import torch \n",
    "import torchvision\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os \n",
    "# Import data\n",
    "from datasets import GetDatasets\n",
    "from utils import utils_model\n",
    "from metrics.metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specification\n",
    "from configs.args_parse import configs\n",
    "import yaml\n",
    "from utils import dict_to_dotdict\n",
    "im_size = 256 # Image resolution\n",
    "\n",
    "#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open(\"configs/demo.yml\", 'r') as file:\n",
    "    config_dict = yaml.safe_load(file)\n",
    "args = dict_to_dotdict(config_dict)\n",
    "\n",
    "lora_checkpoint = \"demo_models/LSUN_RAM_gaussian_deblur.ckpt\"\n",
    "Num_Grad_steps = 3 \n",
    "Num_Diff_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> pre-trained model \n",
    "data = GetDatasets(\n",
    "dataset_dir=\"./demo/demo_data\",\n",
    "im_size=256,\n",
    "dataset_name=\"demo\",\n",
    "transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.CenterCrop((im_size, im_size)),\n",
    "])\n",
    ")\n",
    "print(f\"Dataset length: {len(data)}\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6991a",
   "metadata": {},
   "source": [
    "#### Define forward observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define physics operator for deblurring\n",
    "from physics import Deblurring, Kernels\n",
    "\n",
    "sigma = 0.05 #  Noise level\n",
    "\n",
    "kernel = Kernels(\n",
    "    operator_name=\"anisotropic\",\n",
    "    kernel_size=(5,1),\n",
    "    device=device\n",
    ").get_blur()\n",
    "\n",
    "physics_operator = Deblurring(\n",
    "    sigma_model=sigma,\n",
    "    operator_name=\"gaussian\",\n",
    "    device=device,\n",
    "    scale_image=True\n",
    ")\n",
    "\n",
    "# Plot example data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = next(iter(data_loader)).to(device) # Images loaded in [-1,1] range\n",
    "# x = (x + 1)/2  # Normalize to [0, 1]\n",
    "y = physics_operator.y(x, blur = kernel)  # Apply the physics operator (deblurring)\n",
    "\n",
    "x_pl = (x.permute(0, 2, 3, 1)+1)/2  # Permute to (batch, height, width, channels)\n",
    "x_pl = x_pl[0].cpu().numpy()  # Convert to numpy for plotting\n",
    "y_pl = (y.permute(0, 2, 3, 1)+1)/2  # Permute to (batch, height, width, channels)\n",
    "y_pl = y_pl[0].cpu().clip(0,1).numpy()  # Convert to numpy for plotting\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(x_pl)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Blurred Image\")\n",
    "plt.imshow(y_pl)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3574206",
   "metadata": {},
   "source": [
    "#### Load the diffusion UNet model\n",
    "- Apply LoRA fine-tuning to the model weights $\\theta_{\\text{attention}}$ within each attention layer such that $\\vartheta_{\\text{attention}} = \\theta_{\\text{attention}} + \\Delta\\theta_{\\text{attention}}$, where $\\Delta\\theta_{\\text{attention}} = U^TU$ for $U\\in \\mathbb{R}^{d\\times r}$ is a low-rank perturbation of the attention weights. For this demo, we use $r=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from models import adapter_lora_model\n",
    "import contextlib\n",
    "\n",
    "print(\"Loading model...\")\n",
    "with contextlib.redirect_stdout(None): # Reduce verbosity of model output\n",
    "    \n",
    "    state_dict = torch.load(lora_checkpoint, map_location=device, weights_only=True)\n",
    "\n",
    "    model = adapter_lora_model(args)\n",
    "    model.load_state_dict(\n",
    "        state_dict[\"state_dict\"],\n",
    "        strict=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "print(\"Model loaded successfully.\")\n",
    "print(\n",
    "    \"\\nLoRA weights:\", sum([p.numel() for k, p in state_dict[\"state_dict\"].items()]),\n",
    "    \"\\nTotal model parameters:\", sum([p.numel() for p in model.parameters()]),\n",
    "    \"\\nLoRA accounts for {:.2f}% of the total model parameters.\".format(\n",
    "        sum([p.numel() for k, p in state_dict[\"state_dict\"].items()]) / sum([p.numel() for p in model.parameters()]) * 100)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cae95",
   "metadata": {},
   "source": [
    "#### Load the Diffusion Schedule \n",
    "\n",
    "The pretrained diffusion model was trained using score matching to predict $x_0$ from the observations\n",
    "$$\n",
    "    x_t \\sim \\mathcal{N}(\\sqrt{\\overline{\\alpha}_t}x_0, (1 - \\overline{\\alpha}_t)I),\n",
    "$$\n",
    "where $\\overline{\\alpha}_t = \\prod_{s=1}^t (1-\\beta_s)$ and $\\beta_s$ is a linear noise schedule ranging from $0.0001$ to $0.02$ over $T=1000$ steps. The diffusion schedule is defined by the parameters $\\overline{\\alpha}_t$ and $\\beta_t$. \n",
    "\n",
    "The following code loads the diffusion schedule and a denoising timestep which is used to sample $x_{t_\\delta}$ from $x_{0,t}$ within the LATINO architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diffusion model \n",
    "from unfolded_models import GetDenoisingTimestep, DiffusionScheduler\n",
    "\n",
    "diffusion_schedule = DiffusionScheduler(device = device)\n",
    "denoising_timestep = GetDenoisingTimestep(scheduler = diffusion_schedule, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f5e448",
   "metadata": {},
   "source": [
    "#### Load the LATINO architecture and Conditional Diffusion Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unrolled posterior sampling loop to sample from x_0 given y and xt\n",
    "from unfolded_models import HQS_models as LATINO_module #\n",
    "\n",
    "unrolled_loop = LATINO_module(\n",
    "    model,\n",
    "    physics_operator,\n",
    "    diffusion_schedule,\n",
    "    denoising_timestep,\n",
    "    args,\n",
    "    device=device,\n",
    "    max_unfolded_iter=Num_Grad_steps,\n",
    ")\n",
    "\n",
    "# Use RAM to initialize the unrolled joint posterior sampling loop\n",
    "use_RAM = False\n",
    "if use_RAM:\n",
    "    # Mirror the physics operator in deepinverse\n",
    "    from deepinv.physics import BlurFFT, GaussianNoise\n",
    "    dinv_physics = BlurFFT(\n",
    "        (3, im_size, im_size),\n",
    "        filter = kernel.unsqueeze(0).unsqueeze(0),\n",
    "        device=device,\n",
    "        noise_model = GaussianNoise(sigma=sigma),\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the distilled conditional diffusion model\n",
    "from runners import Conditional_sampler\n",
    "diffusion = Conditional_sampler(\n",
    "    unrolled_loop,\n",
    "    physics_operator,\n",
    "    diffusion_schedule,\n",
    "    device,\n",
    "    args,\n",
    "    dphys = dinv_physics if use_RAM else None,\n",
    ")\n",
    "\n",
    "if \"HQS_state_dict\" in state_dict.keys():\n",
    "    print(\"Loading HQS state dict...\")\n",
    "    print(f\"Successfully loaded {sum([p.numel() for k, p in state_dict['HQS_state_dict'].items() if k in diffusion.hqs_model.state_dict().keys() and (diffusion.hqs_model.state_dict()[k]-p).abs().sum() != 0])} parameters.\")\n",
    "    diffusion.hqs_model.load_state_dict(\n",
    "        state_dict[\"HQS_state_dict\"],\n",
    "        strict=False\n",
    "    )\n",
    "\n",
    "if use_RAM and \"RAM_state_dict\" in state_dict.keys():\n",
    "    print(\"Loading fine-tuned RAM state dict...\")\n",
    "    print(f\"Successfully loaded {sum([p.numel() for k, p in state_dict['RAM_state_dict'].items() if k in diffusion.RAM.state_dict().keys() and (diffusion.RAM.state_dict()[k]-p).abs().sum() != 0])} parameters.\")\n",
    "    diffusion.RAM.load_state_dict(\n",
    "        state_dict[\"RAM_state_dict\"],\n",
    "        strict=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1b592",
   "metadata": {},
   "source": [
    "#### Perform Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the diffusion sampler on the test data\n",
    "outputs = []\n",
    "for i, x in enumerate(data_loader):\n",
    "    x = x.to(device)\n",
    "    x_true = (x + 1) / 2  # Normalize to [0, 1]\n",
    "    y = physics_operator.y(x, blur=kernel)  # Apply the physics operator (deblurring)\n",
    "    # Sample from the diffusion model\n",
    "    out = diffusion.sampler(\n",
    "        y, \n",
    "        f\"im_{i}\",\n",
    "        num_timesteps=Num_Diff_steps,\n",
    "    )\n",
    "    outputs.append((x_true, y, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497295f",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and visualize the sampled output\n",
    "# Print reconstruction Metrics\n",
    "for x_true, y, out in outputs:\n",
    "    metrics = Metrics(device=device)\n",
    "    psnrs = metrics.psnr_function(x_true, out[\"xstart_pred\"])\n",
    "    plt.figure(layout=\"constrained\")\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"True Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(x_true.permute(0, 2, 3, 1)[0].cpu().numpy())  # Display the true image\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Blurred Image\")\n",
    "    plt.imshow(y.permute(0, 2, 3, 1)[0].cpu().add(1).mul(0.5).clip(0,1).numpy())  # Display the blurred image\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Sampled Image\")\n",
    "    plt.text(0.5, -0.01, f\"PSNR: {psnrs.item():.2f} dB\", ha='center', va='top', transform=plt.gca().transAxes)\n",
    "    sampled_x0 = out[\"xstart_pred\"].permute(0, 2, 3, 1)  # Permute to (batch, height, width, channels)\n",
    "    sampled_x0 = sampled_x0[0].cpu().numpy()   # Convert to numpy for plotting\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(sampled_x0)  # Display the sampled image\n",
    "    \n",
    "\n",
    "\n",
    "    # Plot the progressive image through the diffusion steps\n",
    "    seq,_ = diffusion_schedule.get_seq_progress_seq(Num_Diff_steps)\n",
    "    progression = out[\"progress_list\"]\n",
    "    fig = plt.figure(layout=\"constrained\")\n",
    "    fig.suptitle(r\"Progressive $x_t$ through Diffusion Steps\", y=0.85)\n",
    "    for j, img in enumerate(progression):\n",
    "        ax = fig.add_subplot(1, len(progression), j + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(fr\"$t = {seq[j]}$\")\n",
    "        ax.imshow(img)  # Display the progressive image\n",
    "        metrics = Metrics(device=device)\n",
    "        psnrs = metrics.psnr_function(x_true, torch.tensor(img).unsqueeze(0).permute(0,3,1,2).to(device))\n",
    "        plt.text(0.5, -0.01, f\"PSNR: {psnrs.item():.2f} dB\", ha='center', va='top', transform=plt.gca().transAxes)\n",
    "\n",
    "    # Plot the progressive prediction through the diffusion steps\n",
    "    seq,_ = diffusion_schedule.get_seq_progress_seq(Num_Diff_steps)\n",
    "    progression = out[\"progress_zero\"]\n",
    "    fig = plt.figure(layout=\"constrained\")\n",
    "    fig.suptitle(r\"Progressive $x_0$ through Diffusion Steps\", y=0.85)\n",
    "    for j, img in enumerate(progression):\n",
    "        img = (img.permute(1, 2, 0) + 1)/2\n",
    "        img = img.cpu().clip(0,1).numpy()  # Convert to numpy for plotting\n",
    "        ax = fig.add_subplot(1, len(progression), j + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(fr\"$t = {seq[j]}$\")\n",
    "        ax.imshow(img)  # Display the progressive image\n",
    "        psnrs = metrics.psnr_function(x_true, torch.tensor(img).unsqueeze(0).permute(0,3,1,2).to(device))\n",
    "        plt.text(0.5, -0.01, f\"PSNR: {psnrs.item():.2f} dB\", ha='center', va='top', transform=plt.gca().transAxes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
