2024-08-16 17:54:00,981 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 17:54:02,421 - INFO - datast size: 15768
2024-08-16 17:54:02,948 - INFO - Start training ...
2024-08-16 17:54:07,666 - INFO - Epoch ==== >> 0
2024-08-16 17:57:30,841 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 17:57:32,265 - INFO - datast size: 15768
2024-08-16 17:57:32,769 - INFO - Start training ...
2024-08-16 17:57:37,414 - INFO - Epoch ==== >> 0
2024-08-16 17:57:37,573 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 17:57:37,950 - INFO - loss:  0.0777
2024-08-16 17:57:38,549 - INFO - grad_norm: 0.0824 ----- param_norm: 186.2146
2024-08-16 17:57:38,572 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 17:57:38,577 - INFO - Saving EMA checkpoint for rate 0...
2024-08-16 17:57:39,667 - INFO - Saving EMA checkpoint for rate 0.99...
2024-08-16 17:59:00,490 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 17:59:01,906 - INFO - datast size: 15768
2024-08-16 17:59:02,399 - INFO - Start training ...
2024-08-16 17:59:06,908 - INFO - Epoch ==== >> 0
2024-08-16 17:59:07,063 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 17:59:07,438 - INFO - loss:  0.0833
2024-08-16 17:59:08,013 - INFO - grad_norm: 0.0874 ----- param_norm: 186.9093
2024-08-16 17:59:08,036 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 17:59:08,041 - INFO - Saving EMA checkpoint for rate 0...
2024-08-16 17:59:09,126 - INFO - Saving EMA checkpoint for rate 0.99...
2024-08-16 18:02:01,095 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 18:02:02,509 - INFO - datast size: 15768
2024-08-16 18:02:03,180 - INFO - Start training ...
2024-08-16 18:02:07,646 - INFO - Epoch ==== >> 0
2024-08-16 18:02:07,816 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:02:08,196 - INFO - loss:  0.0883
2024-08-16 18:02:08,786 - INFO - grad_norm: 0.0838 ----- param_norm: 186.0244
2024-08-16 18:02:08,809 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 18:02:08,815 - INFO - Saving EMA checkpoint for rate 0...
2024-08-16 18:02:09,915 - INFO - Saving EMA checkpoint for rate 0.99...
2024-08-16 18:03:00,682 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 18:03:02,104 - INFO - datast size: 15768
2024-08-16 18:03:02,815 - INFO - Start training ...
2024-08-16 18:03:07,100 - INFO - Epoch ==== >> 0
2024-08-16 18:03:07,273 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:03:07,655 - INFO - loss:  0.1545
2024-08-16 18:03:08,205 - INFO - grad_norm: 0.0717 ----- param_norm: 185.5867
2024-08-16 18:03:08,236 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 18:03:08,241 - INFO - Saving EMA checkpoint for rate 0...
2024-08-16 18:03:09,360 - INFO - Saving EMA checkpoint for rate 0.99...
2024-08-16 18:05:31,057 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 18:05:32,477 - INFO - datast size: 15768
2024-08-16 18:05:32,981 - INFO - Start training ...
2024-08-16 18:05:37,521 - INFO - Epoch ==== >> 0
2024-08-16 18:05:37,682 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:05:38,059 - INFO - loss:  0.0934
2024-08-16 18:05:38,645 - INFO - grad_norm: 0.0864 ----- param_norm: 186.6461
2024-08-16 18:05:38,669 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 18:05:38,674 - INFO - Saving EMA checkpoint for rate 0...
2024-08-16 18:05:39,750 - INFO - Saving EMA checkpoint for rate 0.99...
2024-08-16 18:08:00,815 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 18:08:02,243 - INFO - datast size: 15768
2024-08-16 18:08:02,762 - INFO - Start training ...
2024-08-16 18:08:07,174 - INFO - Epoch ==== >> 0
2024-08-16 18:08:07,343 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:08:07,723 - INFO - loss:  0.0846
2024-08-16 18:08:08,322 - INFO - grad_norm: 0.0725 ----- param_norm: 186.0650
2024-08-16 18:08:08,347 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 18:08:08,352 - INFO - Saving EMA checkpoint for rate 0...
2024-08-16 18:08:09,458 - INFO - Saving EMA checkpoint for rate 0.99...
2024-08-16 18:10:00,828 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 18:10:02,286 - INFO - datast size: 15768
2024-08-16 18:10:02,790 - INFO - Start training ...
2024-08-16 18:10:07,247 - INFO - Epoch ==== >> 0
2024-08-16 18:10:07,403 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:10:07,779 - INFO - loss:  0.0800
2024-08-16 18:10:08,382 - INFO - grad_norm: 0.0923 ----- param_norm: 185.9660
2024-08-16 18:10:08,406 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 18:50:01,172 - INFO - device model: {aug_LoRa_model.device} : {device}
2024-08-16 18:50:02,580 - INFO - datast size: 15768
2024-08-16 18:50:03,078 - INFO - Start training ...
2024-08-16 18:50:07,621 - INFO - Epoch ==== >> 0
2024-08-16 18:50:07,800 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:50:08,177 - INFO - loss:  0.0860
2024-08-16 18:50:08,771 - INFO - grad_norm: 0.0867 ----- param_norm: 186.6394
2024-08-16 18:50:08,794 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 18:51:31,150 - INFO - device model: cuda : cuda
2024-08-16 18:51:32,557 - INFO - datast size: 15768
2024-08-16 18:51:33,055 - INFO - Start training ...
2024-08-16 18:51:37,578 - INFO - Epoch ==== >> 0
2024-08-16 18:51:37,761 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:51:38,138 - INFO - loss:  0.0870
2024-08-16 18:51:38,732 - INFO - grad_norm: 0.0996 ----- param_norm: 185.1697
2024-08-16 18:51:38,757 - INFO - epoch:  0 	 batch: 0	 samples: 8
2024-08-16 18:54:01,136 - INFO - device model: cuda : cuda
2024-08-16 18:54:02,545 - INFO - datast size: 15768
2024-08-16 18:54:03,049 - INFO - Start training ...
2024-08-16 18:54:07,569 - INFO - Epoch ==== >> 0
2024-08-16 18:54:07,736 - INFO - 
Epoch ==== >> 0	|	 batch ==== >> 0

2024-08-16 18:54:08,112 - INFO - loss:  0.4455
2024-08-16 18:54:08,702 - INFO - grad_norm: 0.9677 ----- param_norm: 186.1263
2024-08-16 18:54:08,726 - INFO - epoch:  0 	 batch: 0	 samples: 8
