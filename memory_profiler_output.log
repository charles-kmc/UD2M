Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1231.5 MiB   1231.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1596.4 MiB    365.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1596.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1627.2 MiB     30.7 MiB           1           self.optimizer.step()
   278   1627.2 MiB      0.0 MiB           1           if ema_update:
   279   1629.7 MiB      0.1 MiB           2               self.ema.update(
   280   1629.6 MiB      2.5 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1629.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1629.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1629.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1629.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1660.5 MiB   1660.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1660.8 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1660.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1660.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1660.8 MiB      0.0 MiB           1           if ema_update:
   279   1660.8 MiB      0.0 MiB           2               self.ema.update(
   280   1660.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1660.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1660.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1660.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1660.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1678.5 MiB   1678.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1678.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1678.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1678.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1678.5 MiB      0.0 MiB           1           if ema_update:
   279   1678.5 MiB      0.0 MiB           2               self.ema.update(
   280   1678.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1678.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1678.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1678.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1678.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1678.5 MiB   1678.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1678.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1678.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1678.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1678.5 MiB      0.0 MiB           1           if ema_update:
   279   1678.5 MiB      0.0 MiB           2               self.ema.update(
   280   1678.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1678.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1678.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1678.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1678.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1679.6 MiB   1679.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1679.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1679.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1679.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1679.6 MiB      0.0 MiB           1           if ema_update:
   279   1679.6 MiB      0.0 MiB           2               self.ema.update(
   280   1679.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1679.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1679.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1679.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1679.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1679.6 MiB   1679.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1679.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1679.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1679.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1679.6 MiB      0.0 MiB           1           if ema_update:
   279   1679.6 MiB      0.0 MiB           2               self.ema.update(
   280   1679.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1679.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1679.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1679.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1679.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.6 MiB   1685.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.6 MiB      0.0 MiB           1           if ema_update:
   279   1685.6 MiB      0.0 MiB           2               self.ema.update(
   280   1685.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.6 MiB   1685.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1644.5 MiB    -41.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1644.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1644.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1644.5 MiB      0.0 MiB           1           if ema_update:
   279   1644.5 MiB      0.0 MiB           2               self.ema.update(
   280   1644.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1644.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1644.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1644.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1644.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1663.4 MiB   1663.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1663.4 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1663.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1663.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1663.4 MiB      0.0 MiB           1           if ema_update:
   279   1663.4 MiB      0.0 MiB           2               self.ema.update(
   280   1663.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1663.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1663.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1663.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1663.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1669.4 MiB   1669.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1669.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1669.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1669.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1669.4 MiB      0.0 MiB           1           if ema_update:
   279   1669.4 MiB      0.0 MiB           2               self.ema.update(
   280   1669.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1669.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1669.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1669.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1669.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1675.4 MiB   1675.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1675.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1675.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1675.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1675.4 MiB      0.0 MiB           1           if ema_update:
   279   1675.4 MiB      0.0 MiB           2               self.ema.update(
   280   1675.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1675.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1675.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1675.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1675.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.6 MiB   1648.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.6 MiB      0.0 MiB           1           if ema_update:
   279   1648.6 MiB      0.0 MiB           2               self.ema.update(
   280   1648.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.4 MiB   1684.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.4 MiB      0.0 MiB           1           if ema_update:
   279   1684.4 MiB      0.0 MiB           2               self.ema.update(
   280   1684.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.2 MiB   1685.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.2 MiB      0.0 MiB           1           if ema_update:
   279   1685.2 MiB      0.0 MiB           2               self.ema.update(
   280   1685.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.5 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.5 MiB      0.0 MiB           1           if ema_update:
   279   1686.5 MiB      0.0 MiB           2               self.ema.update(
   280   1686.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.5 MiB   1686.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.5 MiB      0.0 MiB           1           if ema_update:
   279   1686.5 MiB      0.0 MiB           2               self.ema.update(
   280   1686.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.5 MiB   1686.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.5 MiB      0.0 MiB           1           if ema_update:
   279   1686.5 MiB      0.0 MiB           2               self.ema.update(
   280   1686.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.5 MiB   1686.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.5 MiB      0.0 MiB           1           if ema_update:
   279   1686.5 MiB      0.0 MiB           2               self.ema.update(
   280   1686.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.5 MiB   1686.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.5 MiB      0.0 MiB           1           if ema_update:
   279   1686.5 MiB      0.0 MiB           2               self.ema.update(
   280   1686.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.4 MiB   1648.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.4 MiB      0.0 MiB           1           if ema_update:
   279   1648.4 MiB      0.0 MiB           2               self.ema.update(
   280   1648.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.2 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.2 MiB      0.0 MiB           1           if ema_update:
   279   1684.2 MiB      0.0 MiB           2               self.ema.update(
   280   1684.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.4 MiB   1648.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.4 MiB      0.0 MiB           1           if ema_update:
   279   1648.4 MiB      0.0 MiB           2               self.ema.update(
   280   1648.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.2 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.2 MiB      0.0 MiB           1           if ema_update:
   279   1684.2 MiB      0.0 MiB           2               self.ema.update(
   280   1684.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.5 MiB   1686.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.5 MiB      0.0 MiB           1           if ema_update:
   279   1686.5 MiB      0.0 MiB           2               self.ema.update(
   280   1686.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.5 MiB   1686.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.5 MiB      0.0 MiB           1           if ema_update:
   279   1686.5 MiB      0.0 MiB           2               self.ema.update(
   280   1686.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.6 MiB   1686.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.7 MiB      0.0 MiB           1           if ema_update:
   279   1686.7 MiB      0.0 MiB           2               self.ema.update(
   280   1686.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1667.8 MiB   1667.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1667.9 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1667.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1667.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1667.9 MiB      0.0 MiB           1           if ema_update:
   279   1667.9 MiB      0.0 MiB           2               self.ema.update(
   280   1667.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1667.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1667.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1667.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1667.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.9 MiB   1685.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.9 MiB      0.0 MiB           1           if ema_update:
   279   1685.9 MiB      0.0 MiB           2               self.ema.update(
   280   1685.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.9 MiB   1685.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.9 MiB      0.0 MiB           1           if ema_update:
   279   1685.9 MiB      0.0 MiB           2               self.ema.update(
   280   1685.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.9 MiB   1685.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.9 MiB      0.0 MiB           1           if ema_update:
   279   1685.9 MiB      0.0 MiB           2               self.ema.update(
   280   1685.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.9 MiB   1685.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.9 MiB      0.0 MiB           1           if ema_update:
   279   1685.9 MiB      0.0 MiB           2               self.ema.update(
   280   1685.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.9 MiB   1685.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.9 MiB      0.0 MiB           1           if ema_update:
   279   1685.9 MiB      0.0 MiB           2               self.ema.update(
   280   1685.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.9 MiB   1685.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.9 MiB      0.0 MiB           1           if ema_update:
   279   1685.9 MiB      0.0 MiB           2               self.ema.update(
   280   1685.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.4 MiB      0.0 MiB           1           if ema_update:
   279   1687.4 MiB      0.0 MiB           2               self.ema.update(
   280   1687.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.4 MiB   1687.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1632.2 MiB    -55.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1632.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1632.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1632.3 MiB      0.0 MiB           1           if ema_update:
   279   1632.3 MiB      0.0 MiB           2               self.ema.update(
   280   1632.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1632.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1632.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1632.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1632.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1676.7 MiB   1676.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1676.8 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1676.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1676.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1676.8 MiB      0.0 MiB           1           if ema_update:
   279   1676.8 MiB      0.0 MiB           2               self.ema.update(
   280   1676.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1676.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1676.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1676.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1676.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1680.1 MiB   1680.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1680.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1680.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1680.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1680.1 MiB      0.0 MiB           1           if ema_update:
   279   1680.1 MiB      0.0 MiB           2               self.ema.update(
   280   1680.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1680.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1680.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1680.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1680.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1683.1 MiB   1683.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1683.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1683.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1683.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1683.1 MiB      0.0 MiB           1           if ema_update:
   279   1683.1 MiB      0.0 MiB           2               self.ema.update(
   280   1683.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1683.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1683.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1683.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1683.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1689.0 MiB   1689.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1689.1 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1689.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1689.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1689.1 MiB      0.0 MiB           1           if ema_update:
   279   1689.1 MiB      0.0 MiB           2               self.ema.update(
   280   1689.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1689.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1689.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1689.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1689.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1689.1 MiB   1689.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1689.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1689.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1689.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1689.1 MiB      0.0 MiB           1           if ema_update:
   279   1689.1 MiB      0.0 MiB           2               self.ema.update(
   280   1689.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1689.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1689.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1689.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1689.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1689.1 MiB   1689.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1689.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1689.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1689.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1689.1 MiB      0.0 MiB           1           if ema_update:
   279   1689.1 MiB      0.0 MiB           2               self.ema.update(
   280   1689.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1689.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1689.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1689.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1689.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1689.1 MiB   1689.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1689.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1689.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1689.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1689.1 MiB      0.0 MiB           1           if ema_update:
   279   1689.1 MiB      0.0 MiB           2               self.ema.update(
   280   1689.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1689.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1689.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1689.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1689.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.8 MiB   1648.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.8 MiB      0.0 MiB           1           if ema_update:
   279   1648.8 MiB      0.0 MiB           2               self.ema.update(
   280   1648.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.6 MiB   1684.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.6 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.6 MiB      0.0 MiB           1           if ema_update:
   279   1684.6 MiB      0.0 MiB           2               self.ema.update(
   280   1684.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.5 MiB   1687.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.6 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.6 MiB      0.0 MiB           1           if ema_update:
   279   1687.6 MiB      0.0 MiB           2               self.ema.update(
   280   1687.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1693.6 MiB   1693.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1693.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1693.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1693.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1693.6 MiB      0.0 MiB           1           if ema_update:
   279   1693.6 MiB      0.0 MiB           2               self.ema.update(
   280   1693.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1693.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1693.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1693.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1693.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1693.6 MiB   1693.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1693.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1693.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1693.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1693.6 MiB      0.0 MiB           1           if ema_update:
   279   1693.6 MiB      0.0 MiB           2               self.ema.update(
   280   1693.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1693.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1693.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1693.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1693.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1655.8 MiB   1655.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1655.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1655.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1655.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1655.8 MiB      0.0 MiB           1           if ema_update:
   279   1655.8 MiB      0.0 MiB           2               self.ema.update(
   280   1655.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1655.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1655.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1655.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1655.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1679.7 MiB   1679.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1679.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1679.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1679.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1679.7 MiB      0.0 MiB           1           if ema_update:
   279   1679.7 MiB      0.0 MiB           2               self.ema.update(
   280   1679.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1679.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1679.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1679.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1679.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.7 MiB   1687.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.7 MiB      0.0 MiB           1           if ema_update:
   279   1687.7 MiB      0.0 MiB           2               self.ema.update(
   280   1687.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1649.5 MiB   1649.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1649.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1649.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1649.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1649.5 MiB      0.0 MiB           1           if ema_update:
   279   1649.5 MiB      0.0 MiB           2               self.ema.update(
   280   1649.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1649.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1649.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1649.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1649.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.4 MiB   1685.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.4 MiB      0.0 MiB           1           if ema_update:
   279   1685.4 MiB      0.0 MiB           2               self.ema.update(
   280   1685.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.2 MiB   1686.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.2 MiB      0.0 MiB           1           if ema_update:
   279   1686.2 MiB      0.0 MiB           2               self.ema.update(
   280   1686.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.2 MiB   1692.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.2 MiB      0.0 MiB           1           if ema_update:
   279   1692.2 MiB      0.0 MiB           2               self.ema.update(
   280   1692.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.2 MiB   1692.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.2 MiB      0.0 MiB           1           if ema_update:
   279   1692.2 MiB      0.0 MiB           2               self.ema.update(
   280   1692.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.2 MiB   1692.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.2 MiB      0.0 MiB           1           if ema_update:
   279   1692.2 MiB      0.0 MiB           2               self.ema.update(
   280   1692.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.2 MiB   1692.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.2 MiB      0.0 MiB           1           if ema_update:
   279   1692.2 MiB      0.0 MiB           2               self.ema.update(
   280   1692.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1650.3 MiB   1650.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1650.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1650.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1650.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1650.3 MiB      0.0 MiB           1           if ema_update:
   279   1650.3 MiB      0.0 MiB           2               self.ema.update(
   280   1650.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1650.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1650.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1650.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1650.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.1 MiB   1686.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.2 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.2 MiB      0.0 MiB           1           if ema_update:
   279   1686.2 MiB      0.0 MiB           2               self.ema.update(
   280   1686.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.4 MiB   1691.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.4 MiB      0.0 MiB           1           if ema_update:
   279   1691.4 MiB      0.0 MiB           2               self.ema.update(
   280   1691.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.4 MiB   1691.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.4 MiB      0.0 MiB           1           if ema_update:
   279   1691.4 MiB      0.0 MiB           2               self.ema.update(
   280   1691.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.1 MiB   1648.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.1 MiB      0.0 MiB           1           if ema_update:
   279   1648.1 MiB      0.0 MiB           2               self.ema.update(
   280   1648.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1683.9 MiB   1683.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1683.9 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1683.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1683.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1683.9 MiB      0.0 MiB           1           if ema_update:
   279   1683.9 MiB      0.0 MiB           2               self.ema.update(
   280   1683.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1683.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1683.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1683.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1683.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.7 MiB   1684.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.7 MiB      0.0 MiB           1           if ema_update:
   279   1684.7 MiB      0.0 MiB           2               self.ema.update(
   280   1684.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.7 MiB   1684.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.7 MiB      0.0 MiB           1           if ema_update:
   279   1684.7 MiB      0.0 MiB           2               self.ema.update(
   280   1684.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.7 MiB   1684.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.7 MiB      0.0 MiB           1           if ema_update:
   279   1684.7 MiB      0.0 MiB           2               self.ema.update(
   280   1684.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.9 MiB      0.0 MiB           1           if ema_update:
   279   1686.9 MiB      0.0 MiB           2               self.ema.update(
   280   1686.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.9 MiB   1686.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1631.2 MiB   1631.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1633.1 MiB      1.9 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1633.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1633.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1633.1 MiB      0.0 MiB           1           if ema_update:
   279   1633.1 MiB      0.0 MiB           2               self.ema.update(
   280   1633.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1633.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1633.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1633.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1633.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1681.6 MiB   1681.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1681.7 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1681.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1681.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1681.7 MiB      0.0 MiB           1           if ema_update:
   279   1681.7 MiB      0.0 MiB           2               self.ema.update(
   280   1681.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1681.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1681.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1681.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1681.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.8 MiB   1686.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.1 MiB   1648.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.1 MiB      0.0 MiB           1           if ema_update:
   279   1648.1 MiB      0.0 MiB           2               self.ema.update(
   280   1648.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.0 MiB      0.0 MiB           1           if ema_update:
   279   1684.0 MiB      0.0 MiB           2               self.ema.update(
   280   1684.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.8 MiB   1686.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.1 MiB    -38.9 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.1 MiB      0.0 MiB           1           if ema_update:
   279   1648.1 MiB      0.0 MiB           2               self.ema.update(
   280   1648.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1683.9 MiB   1683.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.0 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.0 MiB      0.0 MiB           1           if ema_update:
   279   1684.0 MiB      0.0 MiB           2               self.ema.update(
   280   1684.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.0 MiB      0.0 MiB           1           if ema_update:
   279   1684.0 MiB      0.0 MiB           2               self.ema.update(
   280   1684.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.0 MiB   1690.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.0 MiB      0.0 MiB           1           if ema_update:
   279   1690.0 MiB      0.0 MiB           2               self.ema.update(
   280   1690.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.0 MiB   1690.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.0 MiB      0.0 MiB           1           if ema_update:
   279   1690.0 MiB      0.0 MiB           2               self.ema.update(
   280   1690.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1649.6 MiB   1649.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1649.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1649.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1649.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1649.6 MiB      0.0 MiB           1           if ema_update:
   279   1649.6 MiB      0.0 MiB           2               self.ema.update(
   280   1649.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1649.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1649.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1649.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1649.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.4 MiB   1685.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.5 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.5 MiB      0.0 MiB           1           if ema_update:
   279   1685.5 MiB      0.0 MiB           2               self.ema.update(
   280   1685.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.9 MiB   1648.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.9 MiB      0.0 MiB           1           if ema_update:
   279   1648.9 MiB      0.0 MiB           2               self.ema.update(
   280   1648.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.8 MiB      0.0 MiB           1           if ema_update:
   279   1684.8 MiB      0.0 MiB           2               self.ema.update(
   280   1684.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.8 MiB      0.0 MiB           1           if ema_update:
   279   1684.8 MiB      0.0 MiB           2               self.ema.update(
   280   1684.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.8 MiB      0.0 MiB           1           if ema_update:
   279   1684.8 MiB      0.0 MiB           2               self.ema.update(
   280   1684.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.8 MiB      0.0 MiB           1           if ema_update:
   279   1684.8 MiB      0.0 MiB           2               self.ema.update(
   280   1684.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.8 MiB      0.0 MiB           1           if ema_update:
   279   1684.8 MiB      0.0 MiB           2               self.ema.update(
   280   1684.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.0 MiB      0.0 MiB           1           if ema_update:
   279   1687.0 MiB      0.0 MiB           2               self.ema.update(
   280   1687.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.6 MiB   1687.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.8 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.8 MiB      0.0 MiB           1           if ema_update:
   279   1687.8 MiB      0.0 MiB           2               self.ema.update(
   280   1687.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.8 MiB   1687.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.8 MiB      0.0 MiB           1           if ema_update:
   279   1687.8 MiB      0.0 MiB           2               self.ema.update(
   280   1687.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.2 MiB      0.0 MiB           1           if ema_update:
   279   1648.2 MiB      0.0 MiB           2               self.ema.update(
   280   1648.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.0 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.0 MiB      0.0 MiB           1           if ema_update:
   279   1684.0 MiB      0.0 MiB           2               self.ema.update(
   280   1684.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.3 MiB      0.0 MiB           1           if ema_update:
   279   1686.3 MiB      0.0 MiB           2               self.ema.update(
   280   1686.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.3 MiB      0.0 MiB           1           if ema_update:
   279   1686.3 MiB      0.0 MiB           2               self.ema.update(
   280   1686.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.3 MiB      0.0 MiB           1           if ema_update:
   279   1686.3 MiB      0.0 MiB           2               self.ema.update(
   280   1686.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1691.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1691.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1691.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1691.3 MiB      0.0 MiB           1           if ema_update:
   279   1691.3 MiB      0.0 MiB           2               self.ema.update(
   280   1691.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1691.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1691.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1691.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1691.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1630.4 MiB    -17.8 MiB           1           self.optimizer.step()
   278   1630.4 MiB      0.0 MiB           1           if ema_update:
   279   1630.4 MiB      0.0 MiB           2               self.ema.update(
   280   1630.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1630.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1630.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1630.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1630.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1683.5 MiB   1683.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1683.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1683.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1683.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1683.5 MiB      0.0 MiB           1           if ema_update:
   279   1683.5 MiB      0.0 MiB           2               self.ema.update(
   280   1683.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1683.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1683.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1683.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1683.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.3 MiB      0.0 MiB           1           if ema_update:
   279   1686.3 MiB      0.0 MiB           2               self.ema.update(
   280   1686.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.3 MiB      0.0 MiB           1           if ema_update:
   279   1686.3 MiB      0.0 MiB           2               self.ema.update(
   280   1686.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.3 MiB      0.0 MiB           1           if ema_update:
   279   1686.3 MiB      0.0 MiB           2               self.ema.update(
   280   1686.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.3 MiB      0.0 MiB           1           if ema_update:
   279   1686.3 MiB      0.0 MiB           2               self.ema.update(
   280   1686.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.2 MiB      0.0 MiB           1           if ema_update:
   279   1648.2 MiB      0.0 MiB           2               self.ema.update(
   280   1648.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.1 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.1 MiB      0.0 MiB           1           if ema_update:
   279   1684.1 MiB      0.0 MiB           2               self.ema.update(
   280   1684.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1651.2 MiB   1651.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1651.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1651.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1651.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1651.2 MiB      0.0 MiB           1           if ema_update:
   279   1651.2 MiB      0.0 MiB           2               self.ema.update(
   280   1651.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1651.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1651.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1651.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1651.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.1 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.1 MiB      0.0 MiB           1           if ema_update:
   279   1687.1 MiB      0.0 MiB           2               self.ema.update(
   280   1687.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1649.7 MiB   1649.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1649.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1649.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1649.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1649.7 MiB      0.0 MiB           1           if ema_update:
   279   1649.7 MiB      0.0 MiB           2               self.ema.update(
   280   1649.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1649.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1649.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1649.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1649.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.4 MiB   1685.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.6 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.6 MiB      0.0 MiB           1           if ema_update:
   279   1685.6 MiB      0.0 MiB           2               self.ema.update(
   280   1685.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.2 MiB      0.0 MiB           1           if ema_update:
   279   1648.2 MiB      0.0 MiB           2               self.ema.update(
   280   1648.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.1 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.1 MiB      0.0 MiB           1           if ema_update:
   279   1684.1 MiB      0.0 MiB           2               self.ema.update(
   280   1684.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.1 MiB   1687.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.1 MiB      0.0 MiB           1           if ema_update:
   279   1687.1 MiB      0.0 MiB           2               self.ema.update(
   280   1687.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.1 MiB   1687.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.1 MiB      0.0 MiB           1           if ema_update:
   279   1687.1 MiB      0.0 MiB           2               self.ema.update(
   280   1687.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.2 MiB      0.0 MiB           1           if ema_update:
   279   1648.2 MiB      0.0 MiB           2               self.ema.update(
   280   1648.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.1 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.1 MiB      0.0 MiB           1           if ema_update:
   279   1684.1 MiB      0.0 MiB           2               self.ema.update(
   280   1684.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.1 MiB      0.0 MiB           1           if ema_update:
   279   1684.1 MiB      0.0 MiB           2               self.ema.update(
   280   1684.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1689.9 MiB   1689.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1690.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1690.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1690.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1690.1 MiB      0.0 MiB           1           if ema_update:
   279   1690.1 MiB      0.0 MiB           2               self.ema.update(
   280   1690.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1690.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1690.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1690.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1690.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1649.7 MiB   1649.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1649.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1649.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1649.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1649.7 MiB      0.0 MiB           1           if ema_update:
   279   1649.7 MiB      0.0 MiB           2               self.ema.update(
   280   1649.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1649.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1649.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1649.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1649.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.5 MiB   1685.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.6 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.6 MiB      0.0 MiB           1           if ema_update:
   279   1685.6 MiB      0.0 MiB           2               self.ema.update(
   280   1685.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1694.4 MiB   1694.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1694.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1694.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1694.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1694.4 MiB      0.0 MiB           1           if ema_update:
   279   1694.4 MiB      0.0 MiB           2               self.ema.update(
   280   1694.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1694.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1694.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1694.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1694.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1668.5 MiB   1668.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1668.5 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1668.5 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1668.5 MiB      0.0 MiB           1           self.optimizer.step()
   278   1668.5 MiB      0.0 MiB           1           if ema_update:
   279   1668.5 MiB      0.0 MiB           2               self.ema.update(
   280   1668.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1668.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1668.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1668.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1668.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1683.3 MiB   1683.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1683.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1683.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1683.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1683.4 MiB      0.0 MiB           1           if ema_update:
   279   1683.4 MiB      0.0 MiB           2               self.ema.update(
   280   1683.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1683.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1683.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1683.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1683.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1689.4 MiB   1689.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1689.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1689.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1689.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1689.4 MiB      0.0 MiB           1           if ema_update:
   279   1689.4 MiB      0.0 MiB           2               self.ema.update(
   280   1689.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1689.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1689.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1689.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1689.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1689.4 MiB   1689.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1689.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1689.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1689.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1689.4 MiB      0.0 MiB           1           if ema_update:
   279   1689.4 MiB      0.0 MiB           2               self.ema.update(
   280   1689.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1689.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1689.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1689.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1689.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1649.7 MiB   1649.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1649.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1649.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1649.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1649.7 MiB      0.0 MiB           1           if ema_update:
   279   1649.7 MiB      0.0 MiB           2               self.ema.update(
   280   1649.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1649.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1649.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1649.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1649.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.5 MiB   1685.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.6 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.6 MiB      0.0 MiB           1           if ema_update:
   279   1685.6 MiB      0.0 MiB           2               self.ema.update(
   280   1685.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1686.4 MiB   1686.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1686.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1686.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1686.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1686.4 MiB      0.0 MiB           1           if ema_update:
   279   1686.4 MiB      0.0 MiB           2               self.ema.update(
   280   1686.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1686.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1686.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1686.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1686.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.4 MiB      0.0 MiB           1           if ema_update:
   279   1692.4 MiB      0.0 MiB           2               self.ema.update(
   280   1692.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.4 MiB      0.0 MiB           1           if ema_update:
   279   1692.4 MiB      0.0 MiB           2               self.ema.update(
   280   1692.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.4 MiB      0.0 MiB           1           if ema_update:
   279   1692.4 MiB      0.0 MiB           2               self.ema.update(
   280   1692.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.4 MiB      0.0 MiB           1           if ema_update:
   279   1692.4 MiB      0.0 MiB           2               self.ema.update(
   280   1692.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.4 MiB      0.0 MiB           1           if ema_update:
   279   1692.4 MiB      0.0 MiB           2               self.ema.update(
   280   1692.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1692.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1692.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1692.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1692.4 MiB      0.0 MiB           1           if ema_update:
   279   1692.4 MiB      0.0 MiB           2               self.ema.update(
   280   1692.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1692.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1692.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1692.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1692.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1649.0 MiB   1649.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1649.0 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1649.0 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1649.0 MiB      0.0 MiB           1           self.optimizer.step()
   278   1649.0 MiB      0.0 MiB           1           if ema_update:
   279   1649.0 MiB      0.0 MiB           2               self.ema.update(
   280   1649.0 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1649.0 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1649.0 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1649.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1649.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.9 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.9 MiB      0.0 MiB           1           if ema_update:
   279   1684.9 MiB      0.0 MiB           2               self.ema.update(
   280   1684.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.7 MiB   1687.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.2 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB      0.0 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1688.6 MiB   1688.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1688.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1688.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1688.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1688.6 MiB      0.0 MiB           1           if ema_update:
   279   1688.6 MiB      0.0 MiB           2               self.ema.update(
   280   1688.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1688.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1688.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1688.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1688.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1631.4 MiB   1631.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1632.8 MiB      1.3 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1632.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1632.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1632.8 MiB      0.0 MiB           1           if ema_update:
   279   1632.9 MiB      0.1 MiB           2               self.ema.update(
   280   1632.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1632.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1632.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1632.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1632.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.9 MiB      0.0 MiB           1           if ema_update:
   279   1684.9 MiB      0.0 MiB           2               self.ema.update(
   280   1684.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.5 MiB   1685.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.6 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.6 MiB      0.0 MiB           1           if ema_update:
   279   1685.6 MiB      0.0 MiB           2               self.ema.update(
   280   1685.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.6 MiB   1685.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.6 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.6 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.6 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.6 MiB      0.0 MiB           1           if ema_update:
   279   1685.6 MiB      0.0 MiB           2               self.ema.update(
   280   1685.6 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.6 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.6 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.6 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.6 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.3 MiB   1648.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.3 MiB      0.0 MiB           1           if ema_update:
   279   1648.3 MiB      0.0 MiB           2               self.ema.update(
   280   1648.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.1 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.1 MiB      0.0 MiB           1           if ema_update:
   279   1684.1 MiB      0.0 MiB           2               self.ema.update(
   280   1684.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.3 MiB   1648.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.3 MiB      0.0 MiB           1           if ema_update:
   279   1648.3 MiB      0.0 MiB           2               self.ema.update(
   280   1648.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.1 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.1 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.1 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.1 MiB      0.0 MiB           1           if ema_update:
   279   1684.1 MiB      0.0 MiB           2               self.ema.update(
   280   1684.1 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.1 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.1 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.1 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.1 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1648.3 MiB   1648.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1648.3 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1648.3 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1648.3 MiB      0.0 MiB           1           self.optimizer.step()
   278   1648.3 MiB      0.0 MiB           1           if ema_update:
   279   1648.3 MiB      0.0 MiB           2               self.ema.update(
   280   1648.3 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1648.3 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1648.3 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1648.3 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1648.3 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.2 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.2 MiB      0.0 MiB           1           if ema_update:
   279   1684.2 MiB      0.0 MiB           2               self.ema.update(
   280   1684.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1688.6 MiB   1688.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1688.8 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1688.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1647.5 MiB    -41.2 MiB           1           self.optimizer.step()
   278   1647.5 MiB      0.0 MiB           1           if ema_update:
   279   1647.5 MiB      0.0 MiB           2               self.ema.update(
   280   1647.5 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1647.5 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1647.5 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1647.5 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1647.5 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1683.4 MiB   1683.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1683.4 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1683.4 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1683.4 MiB      0.0 MiB           1           self.optimizer.step()
   278   1683.4 MiB      0.0 MiB           1           if ema_update:
   279   1683.4 MiB      0.0 MiB           2               self.ema.update(
   280   1683.4 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1683.4 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1683.4 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1683.4 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1683.4 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.9 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.9 MiB      0.0 MiB           1           if ema_update:
   279   1687.9 MiB    -38.9 MiB           2               self.ema.update(
   280   1687.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1649.0 MiB    -38.9 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1649.0 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1649.0 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1684.9 MiB      0.1 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1684.9 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1684.9 MiB      0.0 MiB           1           self.optimizer.step()
   278   1684.9 MiB      0.0 MiB           1           if ema_update:
   279   1684.9 MiB      0.0 MiB           2               self.ema.update(
   280   1684.9 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1684.9 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1684.9 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1684.9 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1684.9 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1685.7 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1685.7 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1685.7 MiB      0.0 MiB           1           self.optimizer.step()
   278   1685.7 MiB      0.0 MiB           1           if ema_update:
   279   1685.7 MiB      0.0 MiB           2               self.ema.update(
   280   1685.7 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1685.7 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1685.7 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1685.7 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1685.7 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1667.8 MiB   1667.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1667.8 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1667.8 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1667.8 MiB      0.0 MiB           1           self.optimizer.step()
   278   1667.8 MiB      0.0 MiB           1           if ema_update:
   279   1667.8 MiB      0.0 MiB           2               self.ema.update(
   280   1667.8 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1667.8 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1667.8 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1667.8 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1667.8 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1681.2 MiB   1681.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1681.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1681.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1681.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1681.2 MiB      0.0 MiB           1           if ema_update:
   279   1681.2 MiB      0.0 MiB           2               self.ema.update(
   280   1681.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1681.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1681.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1681.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1681.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
   276   1687.2 MiB      0.0 MiB           1           ema_update = True#optimizer_step(self.optimizer, self.model_params_learnable, self.logger)
   277   1687.2 MiB      0.0 MiB           1           self.optimizer.step()
   278   1687.2 MiB      0.0 MiB           1           if ema_update:
   279   1687.2 MiB      0.0 MiB           2               self.ema.update(
   280   1687.2 MiB      0.0 MiB           1                           self.accelerator.unwrap_model(self.model), 
   281   1687.2 MiB      0.0 MiB           1                           self.ema_rate
   282                                                                 )
   283                                                 
   284                                                 # set grad to zero
   285   1687.2 MiB      0.0 MiB           1           zero_grad(self.model_params_learnable)
   286                                                 
   287                                                 # scheduler    
   288   1687.2 MiB      0.0 MiB           1           self._anneal_lr(epoch)
   289   1687.2 MiB      0.0 MiB           1           self.log_epoch(epoch)


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   268   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+')) 
   269                                             def run_epoch(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op, epoch):
   270                                                 # pass data through forward and backward steps
   271   1687.2 MiB      0.0 MiB           1           self.run_forward_backward(batch_data, batch_sol_Tk, batch_y, blur_kernel_op)
   272                                                 
   273                                                 # optimisation step
   274                                                 # if self.gradient_accumulation:
   275                                                 #     if (self.ii + 1) % self.gradient_accumulation_steps == 0:
elf.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.2 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.8 MiB      0.0 MiB           1                       model_y,
   305   1686.8 MiB      0.0 MiB           1                       batch_data,
   306   1686.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.1 MiB   1648.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.1 MiB      0.0 MiB           1                       model_y,
   305   1648.1 MiB      0.0 MiB           1                       batch_data,
   306   1648.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.0 MiB      0.0 MiB           1                       model_y,
   305   1684.0 MiB      0.0 MiB           1                       batch_data,
   306   1684.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.8 MiB   1686.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.2 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.8 MiB      0.0 MiB           1                       model_y,
   305   1686.8 MiB      0.0 MiB           1                       batch_data,
   306   1686.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.1 MiB    -38.9 MiB           1           self.accelerator.backward(loss)
   339   1648.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1683.9 MiB   1683.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1683.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1683.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.0 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1683.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1683.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1683.9 MiB      0.0 MiB           1                       model_y,
   305   1683.9 MiB      0.0 MiB           1                       batch_data,
   306   1683.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.0 MiB      0.0 MiB           1                       model_y,
   305   1684.0 MiB      0.0 MiB           1                       batch_data,
   306   1684.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.0 MiB   1690.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.0 MiB      0.0 MiB           1                       model_y,
   305   1690.0 MiB      0.0 MiB           1                       batch_data,
   306   1690.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.0 MiB   1690.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.0 MiB      0.0 MiB           1                       model_y,
   305   1690.0 MiB      0.0 MiB           1                       batch_data,
   306   1690.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1649.6 MiB   1649.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1649.6 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1649.6 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1649.6 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1649.6 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1649.6 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1649.6 MiB      0.0 MiB           1                       model_y,
   305   1649.6 MiB      0.0 MiB           1                       batch_data,
   306   1649.6 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1649.6 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1649.6 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1649.6 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1649.6 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1649.6 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1649.6 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1649.6 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1649.6 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1649.6 MiB      0.0 MiB           2           with torch.no_grad():
   332   1649.6 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1649.6 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1649.6 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1649.6 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1649.6 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1649.6 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1649.6 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1685.4 MiB   1685.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1685.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1685.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1685.5 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1685.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1685.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1685.4 MiB      0.0 MiB           1                       model_y,
   305   1685.4 MiB      0.0 MiB           1                       batch_data,
   306   1685.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1685.5 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1685.5 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1685.5 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1685.5 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1685.5 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1685.5 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1685.5 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1685.5 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1685.5 MiB      0.0 MiB           2           with torch.no_grad():
   332   1685.5 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1685.5 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1685.5 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1685.5 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1685.5 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1685.5 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1685.5 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.9 MiB   1648.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.9 MiB      0.0 MiB           1                       model_y,
   305   1648.9 MiB      0.0 MiB           1                       batch_data,
   306   1648.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.6 MiB   1687.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.6 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.6 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.8 MiB      0.2 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.6 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.6 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.6 MiB      0.0 MiB           1                       model_y,
   305   1687.6 MiB      0.0 MiB           1                       batch_data,
   306   1687.6 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.8 MiB   1687.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.8 MiB      0.0 MiB           1                       model_y,
   305   1687.8 MiB      0.0 MiB           1                       batch_data,
   306   1687.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.2 MiB      0.0 MiB           1                       model_y,
   305   1648.2 MiB      0.0 MiB           1                       batch_data,
   306   1648.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.0 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.0 MiB      0.0 MiB           1                       model_y,
   305   1684.0 MiB      0.0 MiB           1                       batch_data,
   306   1684.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.3 MiB      0.0 MiB           1                       model_y,
   305   1686.3 MiB      0.0 MiB           1                       batch_data,
   306   1686.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.3 MiB      0.0 MiB           1                       model_y,
   305   1686.3 MiB      0.0 MiB           1                       batch_data,
   306   1686.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.3 MiB      0.0 MiB           1                       model_y,
   305   1686.3 MiB      0.0 MiB           1                       batch_data,
   306   1686.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1691.3 MiB   1691.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1691.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1691.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1691.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1691.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1691.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1691.3 MiB      0.0 MiB           1                       model_y,
   305   1691.3 MiB      0.0 MiB           1                       batch_data,
   306   1691.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1691.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1691.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1691.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1691.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1691.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1691.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1691.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1691.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1691.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1691.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1691.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1691.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1691.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1691.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1691.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1691.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.2 MiB      0.0 MiB           1                       model_y,
   305   1648.2 MiB      0.0 MiB           1                       batch_data,
   306   1648.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1683.5 MiB   1683.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1683.5 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1683.5 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1683.5 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1683.5 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1683.5 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1683.5 MiB      0.0 MiB           1                       model_y,
   305   1683.5 MiB      0.0 MiB           1                       batch_data,
   306   1683.5 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1683.5 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1683.5 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1683.5 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1683.5 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1683.5 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1683.5 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1683.5 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1683.5 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1683.5 MiB      0.0 MiB           2           with torch.no_grad():
   332   1683.5 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1683.5 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1683.5 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1683.5 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1683.5 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1683.5 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1683.5 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.3 MiB      0.0 MiB           1                       model_y,
   305   1686.3 MiB      0.0 MiB           1                       batch_data,
   306   1686.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.3 MiB      0.0 MiB           1                       model_y,
   305   1686.3 MiB      0.0 MiB           1                       batch_data,
   306   1686.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.3 MiB      0.0 MiB           1                       model_y,
   305   1686.3 MiB      0.0 MiB           1                       batch_data,
   306   1686.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.3 MiB   1686.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.3 MiB      0.0 MiB           1                       model_y,
   305   1686.3 MiB      0.0 MiB           1                       batch_data,
   306   1686.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.2 MiB      0.0 MiB           1                       model_y,
   305   1648.2 MiB      0.0 MiB           1                       batch_data,
   306   1648.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.1 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.0 MiB      0.0 MiB           1                       model_y,
   305   1684.0 MiB      0.0 MiB           1                       batch_data,
   306   1684.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1651.2 MiB   1651.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1651.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1651.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1651.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1651.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1651.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1651.2 MiB      0.0 MiB           1                       model_y,
   305   1651.2 MiB      0.0 MiB           1                       batch_data,
   306   1651.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1651.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1651.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1651.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1651.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1651.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1651.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1651.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1651.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1651.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1651.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1651.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1651.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1651.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1651.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1651.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1651.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.0 MiB   1687.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.1 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.0 MiB      0.0 MiB           1                       model_y,
   305   1687.0 MiB      0.0 MiB           1                       batch_data,
   306   1687.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1649.7 MiB   1649.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1649.7 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1649.7 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1649.7 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1649.7 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1649.7 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1649.7 MiB      0.0 MiB           1                       model_y,
   305   1649.7 MiB      0.0 MiB           1                       batch_data,
   306   1649.7 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1649.7 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1649.7 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1649.7 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1649.7 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1649.7 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1649.7 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1649.7 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1649.7 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1649.7 MiB      0.0 MiB           2           with torch.no_grad():
   332   1649.7 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1649.7 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1649.7 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1649.7 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1649.7 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1649.7 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1649.7 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1685.4 MiB   1685.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1685.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1685.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1685.6 MiB      0.2 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1685.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1685.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1685.4 MiB      0.0 MiB           1                       model_y,
   305   1685.4 MiB      0.0 MiB           1                       batch_data,
   306   1685.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1685.6 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1685.6 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1685.6 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1685.6 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1685.6 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1685.6 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1685.6 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1685.6 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1685.6 MiB      0.0 MiB           2           with torch.no_grad():
   332   1685.6 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1685.6 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1685.6 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1685.6 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1685.6 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1685.6 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1685.6 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.2 MiB      0.0 MiB           1                       model_y,
   305   1648.2 MiB      0.0 MiB           1                       batch_data,
   306   1648.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.1 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.0 MiB      0.0 MiB           1                       model_y,
   305   1684.0 MiB      0.0 MiB           1                       batch_data,
   306   1684.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.1 MiB   1687.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.1 MiB      0.0 MiB           1                       model_y,
   305   1687.1 MiB      0.0 MiB           1                       batch_data,
   306   1687.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.1 MiB   1687.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.1 MiB      0.0 MiB           1                       model_y,
   305   1687.1 MiB      0.0 MiB           1                       batch_data,
   306   1687.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.2 MiB   1648.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.2 MiB      0.0 MiB           1                       model_y,
   305   1648.2 MiB      0.0 MiB           1                       batch_data,
   306   1648.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.0 MiB   1684.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.1 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.0 MiB      0.0 MiB           1                       model_y,
   305   1684.0 MiB      0.0 MiB           1                       batch_data,
   306   1684.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.1 MiB      0.0 MiB           1                       model_y,
   305   1684.1 MiB      0.0 MiB           1                       batch_data,
   306   1684.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1689.9 MiB   1689.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1689.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1689.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.2 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1689.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1689.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1689.9 MiB      0.0 MiB           1                       model_y,
   305   1689.9 MiB      0.0 MiB           1                       batch_data,
   306   1689.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1690.1 MiB   1690.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1690.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1690.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1690.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1690.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1690.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1690.1 MiB      0.0 MiB           1                       model_y,
   305   1690.1 MiB      0.0 MiB           1                       batch_data,
   306   1690.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1690.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1690.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1690.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1690.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1690.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1690.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1690.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1690.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1690.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1690.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1690.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1690.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1690.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1690.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1690.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1690.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1649.7 MiB   1649.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1649.7 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1649.7 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1649.7 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1649.7 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1649.7 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1649.7 MiB      0.0 MiB           1                       model_y,
   305   1649.7 MiB      0.0 MiB           1                       batch_data,
   306   1649.7 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1649.7 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1649.7 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1649.7 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1649.7 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1649.7 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1649.7 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1649.7 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1649.7 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1649.7 MiB      0.0 MiB           2           with torch.no_grad():
   332   1649.7 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1649.7 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1649.7 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1649.7 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1649.7 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1649.7 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1649.7 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1685.5 MiB   1685.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1685.5 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1685.5 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1685.6 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1685.5 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1685.5 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1685.5 MiB      0.0 MiB           1                       model_y,
   305   1685.5 MiB      0.0 MiB           1                       batch_data,
   306   1685.5 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1685.6 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1685.6 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1685.6 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1685.6 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1685.6 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1685.6 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1685.6 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1685.6 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1685.6 MiB      0.0 MiB           2           with torch.no_grad():
   332   1685.6 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1685.6 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1685.6 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1685.6 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1685.6 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1685.6 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1685.6 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1694.4 MiB   1694.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1694.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1694.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1694.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1694.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1694.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1694.4 MiB      0.0 MiB           1                       model_y,
   305   1694.4 MiB      0.0 MiB           1                       batch_data,
   306   1694.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1694.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1694.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1694.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1694.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1694.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1694.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1694.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1694.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1694.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1694.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1694.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1694.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1694.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1694.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1694.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1694.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1668.5 MiB   1668.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1668.5 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1668.5 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1668.5 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1668.5 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1668.5 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1668.5 MiB      0.0 MiB           1                       model_y,
   305   1668.5 MiB      0.0 MiB           1                       batch_data,
   306   1668.5 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1668.5 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1668.5 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1668.5 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1668.5 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1668.5 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1668.5 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1668.5 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1668.5 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1668.5 MiB      0.0 MiB           2           with torch.no_grad():
   332   1668.5 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1668.5 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1668.5 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1668.5 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1668.5 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1668.5 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1668.5 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1683.3 MiB   1683.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1683.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1683.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1683.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1683.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1683.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1683.3 MiB      0.0 MiB           1                       model_y,
   305   1683.3 MiB      0.0 MiB           1                       batch_data,
   306   1683.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1683.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1683.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1683.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1683.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1683.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1683.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1683.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1683.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1683.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1683.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1683.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1683.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1683.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1683.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1683.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1683.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1689.4 MiB   1689.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1689.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1689.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1689.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1689.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1689.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1689.4 MiB      0.0 MiB           1                       model_y,
   305   1689.4 MiB      0.0 MiB           1                       batch_data,
   306   1689.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1689.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1689.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1689.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1689.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1689.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1689.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1689.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1689.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1689.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1689.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1689.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1689.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1689.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1689.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1689.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1689.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1689.4 MiB   1689.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1689.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1689.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1689.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1689.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1689.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1689.4 MiB      0.0 MiB           1                       model_y,
   305   1689.4 MiB      0.0 MiB           1                       batch_data,
   306   1689.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1689.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1689.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1689.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1689.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1689.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1689.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1689.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1689.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1689.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1689.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1689.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1689.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1689.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1689.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1689.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1689.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1649.7 MiB   1649.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1649.7 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1649.7 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1649.7 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1649.7 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1649.7 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1649.7 MiB      0.0 MiB           1                       model_y,
   305   1649.7 MiB      0.0 MiB           1                       batch_data,
   306   1649.7 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1649.7 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1649.7 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1649.7 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1649.7 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1649.7 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1649.7 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1649.7 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1649.7 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1649.7 MiB      0.0 MiB           2           with torch.no_grad():
   332   1649.7 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1649.7 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1649.7 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1649.7 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1649.7 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1649.7 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1649.7 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1685.5 MiB   1685.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1685.5 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1685.5 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1685.6 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1685.5 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1685.5 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1685.5 MiB      0.0 MiB           1                       model_y,
   305   1685.5 MiB      0.0 MiB           1                       batch_data,
   306   1685.5 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1685.6 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1685.6 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1685.6 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1685.6 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1685.6 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1685.6 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1685.6 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1685.6 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1685.6 MiB      0.0 MiB           2           with torch.no_grad():
   332   1685.6 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1685.6 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1685.6 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1685.6 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1685.6 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1685.6 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1685.6 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1686.4 MiB   1686.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1686.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1686.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1686.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1686.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1686.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1686.4 MiB      0.0 MiB           1                       model_y,
   305   1686.4 MiB      0.0 MiB           1                       batch_data,
   306   1686.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1686.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1686.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1686.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1686.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1686.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1686.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1686.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1686.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1686.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1686.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1686.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1686.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1686.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1686.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1686.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1686.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1692.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1692.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1692.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1692.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1692.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1692.4 MiB      0.0 MiB           1                       model_y,
   305   1692.4 MiB      0.0 MiB           1                       batch_data,
   306   1692.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1692.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1692.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1692.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1692.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1692.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1692.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1692.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1692.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1692.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1692.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1692.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1692.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1692.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1692.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1692.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1692.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1692.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1692.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1692.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1692.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1692.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1692.4 MiB      0.0 MiB           1                       model_y,
   305   1692.4 MiB      0.0 MiB           1                       batch_data,
   306   1692.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1692.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1692.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1692.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1692.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1692.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1692.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1692.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1692.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1692.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1692.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1692.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1692.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1692.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1692.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1692.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1692.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1692.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1692.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1692.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1692.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1692.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1692.4 MiB      0.0 MiB           1                       model_y,
   305   1692.4 MiB      0.0 MiB           1                       batch_data,
   306   1692.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1692.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1692.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1692.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1692.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1692.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1692.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1692.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1692.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1692.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1692.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1692.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1692.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1692.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1692.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1692.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1692.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1692.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1692.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1692.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1692.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1692.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1692.4 MiB      0.0 MiB           1                       model_y,
   305   1692.4 MiB      0.0 MiB           1                       batch_data,
   306   1692.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1692.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1692.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1692.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1692.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1692.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1692.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1692.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1692.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1692.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1692.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1692.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1692.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1692.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1692.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1692.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1692.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1692.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1692.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1692.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1692.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1692.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1692.4 MiB      0.0 MiB           1                       model_y,
   305   1692.4 MiB      0.0 MiB           1                       batch_data,
   306   1692.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1692.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1692.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1692.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1692.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1692.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1692.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1692.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1692.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1692.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1692.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1692.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1692.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1692.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1692.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1692.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1692.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1692.4 MiB   1692.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1692.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1692.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1692.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1692.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1692.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1692.4 MiB      0.0 MiB           1                       model_y,
   305   1692.4 MiB      0.0 MiB           1                       batch_data,
   306   1692.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1692.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1692.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1692.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1692.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1692.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1692.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1692.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1692.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1692.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1692.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1692.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1692.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1692.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1692.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1692.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1692.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1649.0 MiB   1649.0 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1649.0 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1649.0 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1649.0 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1649.0 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1649.0 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1649.0 MiB      0.0 MiB           1                       model_y,
   305   1649.0 MiB      0.0 MiB           1                       batch_data,
   306   1649.0 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1649.0 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1649.0 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1649.0 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1649.0 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1649.0 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1649.0 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1649.0 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1649.0 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1649.0 MiB      0.0 MiB           2           with torch.no_grad():
   332   1649.0 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1649.0 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1649.0 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1649.0 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1649.0 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1649.0 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1649.0 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.9 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.7 MiB   1687.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.7 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.7 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.2 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.7 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.7 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.7 MiB      0.0 MiB           1                       model_y,
   305   1687.7 MiB      0.0 MiB           1                       batch_data,
   306   1687.7 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1688.6 MiB   1688.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1688.6 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1688.6 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1688.6 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1688.6 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1688.6 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1688.6 MiB      0.0 MiB           1                       model_y,
   305   1688.6 MiB      0.0 MiB           1                       batch_data,
   306   1688.6 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1688.6 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1688.6 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1688.6 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1688.6 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1688.6 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1688.6 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1688.6 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1688.6 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1688.6 MiB      0.0 MiB           2           with torch.no_grad():
   332   1688.6 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1688.6 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1688.6 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1688.6 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1688.6 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1688.6 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1688.6 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1631.4 MiB   1631.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1631.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1631.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1631.7 MiB      0.2 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1631.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1631.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1631.4 MiB      0.0 MiB           1                       model_y,
   305   1631.4 MiB      0.0 MiB           1                       batch_data,
   306   1631.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1631.7 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1631.7 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1631.9 MiB      0.2 MiB           1           if self.consistency_loss:   
   316   1631.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1632.7 MiB      0.8 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1632.7 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1632.7 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1632.7 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1632.7 MiB      0.0 MiB           2           with torch.no_grad():
   332   1632.7 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1632.7 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1632.7 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1632.8 MiB      0.1 MiB           1           self.accelerator.backward(loss)
   339   1632.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1632.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1632.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1685.5 MiB   1685.5 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1685.5 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1685.5 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1685.6 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1685.5 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1685.5 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1685.5 MiB      0.0 MiB           1                       model_y,
   305   1685.5 MiB      0.0 MiB           1                       batch_data,
   306   1685.5 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1685.6 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1685.6 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1685.6 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1685.6 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1685.6 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1685.6 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1685.6 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1685.6 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1685.6 MiB      0.0 MiB           2           with torch.no_grad():
   332   1685.6 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1685.6 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1685.6 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1685.6 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1685.6 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1685.6 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1685.6 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1685.6 MiB   1685.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1685.6 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1685.6 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1685.6 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1685.6 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1685.6 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1685.6 MiB      0.0 MiB           1                       model_y,
   305   1685.6 MiB      0.0 MiB           1                       batch_data,
   306   1685.6 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1685.6 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1685.6 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1685.6 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1685.6 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1685.6 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1685.6 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1685.6 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1685.6 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1685.6 MiB      0.0 MiB           2           with torch.no_grad():
   332   1685.6 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1685.6 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1685.6 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1685.6 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1685.6 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1685.6 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1685.6 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.3 MiB   1648.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.3 MiB      0.0 MiB           1                       model_y,
   305   1648.3 MiB      0.0 MiB           1                       batch_data,
   306   1648.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.1 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.1 MiB      0.0 MiB           1                       model_y,
   305   1684.1 MiB      0.0 MiB           1                       batch_data,
   306   1684.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.3 MiB   1648.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.3 MiB      0.0 MiB           1                       model_y,
   305   1648.3 MiB      0.0 MiB           1                       batch_data,
   306   1648.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.1 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.1 MiB      0.0 MiB           1                       model_y,
   305   1684.1 MiB      0.0 MiB           1                       batch_data,
   306   1684.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.1 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.1 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.1 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.1 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.1 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.1 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.1 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.1 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.1 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.1 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.1 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.1 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.1 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.1 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.1 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.1 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1648.3 MiB   1648.3 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1648.3 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1648.3 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1648.3 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1648.3 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1648.3 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1648.3 MiB      0.0 MiB           1                       model_y,
   305   1648.3 MiB      0.0 MiB           1                       batch_data,
   306   1648.3 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1648.3 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1648.3 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1648.3 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1648.3 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1648.3 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1648.3 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1648.3 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1648.3 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1648.3 MiB      0.0 MiB           2           with torch.no_grad():
   332   1648.3 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1648.3 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1648.3 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1648.3 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1648.3 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1648.3 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1648.3 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.1 MiB   1684.1 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.1 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.1 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.2 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.1 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.1 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.1 MiB      0.0 MiB           1                       model_y,
   305   1684.1 MiB      0.0 MiB           1                       batch_data,
   306   1684.1 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1688.6 MiB   1688.6 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1688.6 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1688.6 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1688.8 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1688.6 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1688.6 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1688.6 MiB      0.0 MiB           1                       model_y,
   305   1688.6 MiB      0.0 MiB           1                       batch_data,
   306   1688.6 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1688.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1688.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1688.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1688.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1688.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1688.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1688.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1688.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1688.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1688.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1688.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1688.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1688.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1688.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1688.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1688.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1683.4 MiB   1683.4 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1683.4 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1683.4 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1683.4 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1683.4 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1683.4 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1683.4 MiB      0.0 MiB           1                       model_y,
   305   1683.4 MiB      0.0 MiB           1                       batch_data,
   306   1683.4 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1683.4 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1683.4 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1683.4 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1683.4 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1683.4 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1683.4 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1683.4 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1683.4 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1683.4 MiB      0.0 MiB           2           with torch.no_grad():
   332   1683.4 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1683.4 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1683.4 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1683.4 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1683.4 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1683.4 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1683.4 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1684.8 MiB   1684.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1684.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1684.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1684.9 MiB      0.1 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1684.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1684.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1684.8 MiB      0.0 MiB           1                       model_y,
   305   1684.8 MiB      0.0 MiB           1                       batch_data,
   306   1684.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1684.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1684.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1684.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1684.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1684.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1684.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1684.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1684.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1684.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1684.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1684.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1684.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1684.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1684.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1684.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1684.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1685.7 MiB   1685.7 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1685.7 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1685.7 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1685.7 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1685.7 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1685.7 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1685.7 MiB      0.0 MiB           1                       model_y,
   305   1685.7 MiB      0.0 MiB           1                       batch_data,
   306   1685.7 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1685.7 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1685.7 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1685.7 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1685.7 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1685.7 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1685.7 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1685.7 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1685.7 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1685.7 MiB      0.0 MiB           2           with torch.no_grad():
   332   1685.7 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1685.7 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1685.7 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1685.7 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1685.7 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1685.7 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1685.7 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1667.8 MiB   1667.8 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1667.8 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1667.8 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1667.8 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1667.8 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1667.8 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1667.8 MiB      0.0 MiB           1                       model_y,
   305   1667.8 MiB      0.0 MiB           1                       batch_data,
   306   1667.8 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1667.8 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1667.8 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1667.8 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1667.8 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1667.8 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1667.8 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1667.8 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1667.8 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1667.8 MiB      0.0 MiB           2           with torch.no_grad():
   332   1667.8 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1667.8 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1667.8 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1667.8 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1667.8 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1667.8 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1667.8 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1681.2 MiB   1681.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1681.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1681.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1681.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1681.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1681.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1681.2 MiB      0.0 MiB           1                       model_y,
   305   1681.2 MiB      0.0 MiB           1                       batch_data,
   306   1681.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1681.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1681.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1681.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1681.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1681.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1681.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1681.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1681.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1681.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1681.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1681.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1681.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1681.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1681.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1681.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1681.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.2 MiB   1687.2 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.2 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.2 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.2 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.2 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.2 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.2 MiB      0.0 MiB           1                       model_y,
   305   1687.2 MiB      0.0 MiB           1                       batch_data,
   306   1687.2 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.2 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.2 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.2 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.2 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.2 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.2 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.2 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.2 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.2 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.2 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.2 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.2 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.2 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.2 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.2 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.2 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
   299                                                 # - compute the losses
   300   1687.9 MiB      0.0 MiB           1           batch_sol_Tk.requires_grad_(False)
   301   1687.9 MiB      0.0 MiB           3           model_y = lambda x, t: self.model(x, t, batch_sol_Tk)
   302   1687.9 MiB      0.0 MiB           2           compute_losses = functools.partial(
   303   1687.9 MiB      0.0 MiB           1                       self.diffusion.training_losses,   
   304   1687.9 MiB      0.0 MiB           1                       model_y,
   305   1687.9 MiB      0.0 MiB           1                       batch_data,
   306   1687.9 MiB      0.0 MiB           1                       t
   307                                                         ) 
   308                                                 
   309                                                 # computing losses
   310   1687.9 MiB      0.0 MiB           1           losses_x_pred = compute_losses()
   311                                                 
   312                                                 # get the predicted image
   313   1687.9 MiB      0.0 MiB           1           x_pred = losses_x_pred["pred_xstart"]
   314                                                 # consistency loss
   315   1687.9 MiB      0.0 MiB           1           if self.consistency_loss:   
   316   1687.9 MiB      0.0 MiB           1               batch_y.requires_grad_(False)
   317   1687.9 MiB      0.0 MiB           1               losses_x_cons = self.model.consistency_loss(x_pred, batch_y, blur_kernel_op)
   318                                                 else:
   319                                                     losses_x_cons = 0.0
   320                                                     
   321   1687.9 MiB      0.0 MiB           1           loss = losses_x_cons
   322   1687.9 MiB      0.0 MiB           1           if isinstance(self.schedule_sampler, LossAwareSampler):
   323                                                     self.schedule_sampler.update_with_local_losses(
   324                                                         t, losses_x_pred["loss"].detach()
   325                                                     )
   326                                                     
   327   1687.9 MiB      0.0 MiB           1           loss += (losses_x_pred["loss"]*weights).mean()
   328                                                 # log_loss_dict(
   329                                                 #     self.diffusion, t, {"loss": losses_x_pred["loss"]*weights}, self.logger
   330                                                 # )
   331   1687.9 MiB      0.0 MiB           2           with torch.no_grad():
   332   1687.9 MiB      0.0 MiB           1               loss_val = losses_x_pred["loss"].detach().cpu()*weights.cpu()
   333   1687.9 MiB      0.0 MiB           1               self.logger.info(f"Loss: {loss_val.mean().item()}")            
   334                                                     # monitor loss with wandb
   335   1687.9 MiB      0.0 MiB           1               wandb.log({"loss": loss_val.mean().numpy()})
   336                                                 
   337                                                 # back-propagation: here, gradients are computed
   338   1687.9 MiB      0.0 MiB           1           self.accelerator.backward(loss)
   339   1687.9 MiB      0.0 MiB           1           if self.accelerator.sync_gradients:
   340   1687.9 MiB      0.0 MiB           1               self.accelerator.clip_grad_value_(self.model_params_learnable,  self.clip_value)
   341                                                     # self.accelerator.clip_grad_norm_(self.model_params_learnable,   self.max_grad_norm)
   342                                                 
   343   1687.9 MiB      0.0 MiB           1           self.x_pred = x_pred.detach()


Filename: /users/cmk2000/cmk2000/Deep learning models/LLMs/Conditional-Diffusion-Models-for-IVP/models/trainer_accelerate.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   292   1687.9 MiB   1687.9 MiB           1       @profile(stream=open('memory_profiler_output.log', 'w+'))
   293                                             def run_forward_backward(self, batch_data, batch_sol_Tk, batch_y, blur_kernel_op):
   294                                                 # if self.gradient_accumulation and (self.ii + 1) % self.gradient_accumulation_steps == 0:
   295                                                 
   296                                                 # weight from diffusion sampler    
   297   1687.9 MiB      0.0 MiB           1           t, weights = self.schedule_sampler.sample(batch_data.shape[0], dist_util.dev())
   298                                                 
